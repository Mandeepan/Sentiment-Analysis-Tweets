{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk import ngrams\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#input gold price dataset and convert it into time series data\n",
    "rawPriceData=pd.read_csv('/Users/Mandy/Study/SpringBoard/Capstone 2/GoldPriceData-Raw.csv')\n",
    "rawPriceData['Date'] = pd.to_datetime(rawPriceData['Date'])\n",
    "rawPriceData.set_index(rawPriceData['Date'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling weekend/public holiday missing data by taking average.\n",
    "priceData=rawPriceData.resample('D').interpolate()\n",
    "priceData=priceData['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      date  score  language                                               text\n",
      "0  3/31/18      0       NaN  à¸à¸±à¸à¸à¸µà¸à¸µà¹à¸à¹à¸²à¸§à¸à¸£à¸¡à...\n",
      "1  3/31/18     -1       NaN               Donald Trump and Roseanne are living\n",
      "2  3/31/18     -1       NaN                              I know we're all numb\n",
      "3  3/31/18     -1       NaN                              I know we're all numb\n",
      "4  3/31/18      1       NaN  Diaspora culture is thinking you know better t...\n"
     ]
    }
   ],
   "source": [
    "#input tweet dataset\n",
    "rawTweetData=pd.read_csv('/Users/Mandy/Study/SpringBoard/Capstone 2/GoldTweetDataWorkBook.csv',encoding='latin1')\n",
    "print(rawTweetData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "LangDetectException",
     "evalue": "No features in text.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLangDetectException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-93d07a06ab1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrawTweetData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0meachRowText\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrawTweetData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mrawTweetData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meachRowText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawTweetData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/langdetect/detector_factory.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/langdetect/detector.py\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mwhich\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mhighest\u001b[0m \u001b[0mprobability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         '''\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/langdetect/detector.py\u001b[0m in \u001b[0;36mget_probabilities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sort_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/langdetect/detector.py\u001b[0m in \u001b[0;36m_detect_block\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mLangDetectException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCantDetectError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'No features in text.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanglist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLangDetectException\u001b[0m: No features in text."
     ]
    }
   ],
   "source": [
    "#detect non-English tweets and delete those rows\n",
    "for r in rawTweetData.index:\n",
    "    eachRowText=rawTweetData.text[r]\n",
    "    rawTweetData.loc[r,2]=detect(eachRowText)\n",
    "\n",
    "print(rawTweetData.language[0])\n",
    "print(rawTweetData.language[1])\n",
    "print(rawTweetData.language[2])\n",
    "\n",
    "rawTweetData=rawTweetData[rawTweetData.language=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¸à¸±à¸à¸à¸µà¸à¸µà¹à¸à¹à¸²à¸§à¸à¸£à¸¡à¸­à¸¸à¸à¸¢à¸²à¸à¹à¸à¸£à¸µà¸¢à¸¡à¸à¸´à¸à¸­à¹à¸²à¸§à¸¡à¸²à¸«à¸¢à¸²à¸à¸£à¸°à¸à¸²à¸¢à¸­à¸­à¸à¹à¸ à¸à¸±à¹à¸à¹à¸¥à¸à¸à¹à¸²à¸à¸à¸²à¸à¸£à¸±à¸ à¸ªà¸³à¸à¸±à¸à¸à¹à¸²à¸§à¸à¸±à¹à¸§à¹à¸¥à¸à¸¥à¸à¸à¹à¸²à¸§à¹à¸£à¸·à¹à¸­à¸à¸à¸µà¹à¸¡à¸²à¸à¸¡à¸²à¸¢ à¹à¸­à¸à¸µ - âà¸à¹à¸²à¹à¸£à¸²à¹à¸¡à¹à¸à¸³à¹à¸à¸§à¸±à¸à¸à¸µà¹ à¸¡à¸±à¸à¸à¸°à¸ªà¸²à¸¢à¹à¸à¸´à¸à¹à¸â (à¸à¸³à¹à¸«à¹à¸ªà¸±à¸¡à¸ à¸²à¸©à¸à¹à¸à¸­à¸à¸à¹à¸²à¸à¸­à¸à¸´à¸à¸à¸µà¸à¸£à¸¡à¸­à¸¸à¸à¸¢à¸²à¸) world economic forum - à¸à¸£à¸°à¹à¸à¸¨à¹à¸à¸¢à¸à¸³à¸¥à¸±à¸à¸à¸°à¸à¸´à¸à¸«à¸²à¸à¸ªà¸§à¸¢à¸à¸µà¹à¸ªà¸¸à¸à¹à¸à¹à¸¥à¸ à¹à¸à¸·à¹à¸­à¸à¸·à¹à¸à¸à¸·à¸à¸à¸£à¸£à¸¡à¸à¸²à¸à¸´ pic.twitter.com/Hm75PKAZh4\n",
      "Donald Trump and Roseanne are living\n",
      "I know we're all numb\n",
      "I know we're all numb\n",
      "Diaspora culture is thinking you know better than all the people who stayed home combined and thinking you will go back home and change the culture/ economy because youâre the little snowflake that got the VISA\n"
     ]
    }
   ],
   "source": [
    "#replace special characters, such as URLs, usernames, hashtags, pictures with\"URL\",\"USER\",\"HASHTAG\",\"PICTURE\"\n",
    "#also remove all the special characters\n",
    "for row in rawTweetData.index:\n",
    "    eachTweet=rawTweetData.text[row]\n",
    "    eachTweet_url=re.sub(r\"https:// \\S+\",\"URL\",eachTweet)\n",
    "    eachTweet_pic=re.sub(r\"pic.\\S+\",\"PICTURE\",eachTweet_url)\n",
    "    eachTweet_user=re.sub(r\"@\\S+\",\"USER\",eachTweet_pic)\n",
    "    eachTweet_hashtag=re.sub(r\"#\\S+\",\"HASHTAG\",eachTweet_user)\n",
    "    eachTweet_spechar=re.sub(\"[^A-Za-z0-9]+\",\" \",eachTweet_hashtag)\n",
    "    rawTweetData.set_value(row,'Tweet',eachTweet_spechar)\n",
    "print(rawTweetData.text[0])\n",
    "print(rawTweetData.text[1])\n",
    "print(rawTweetData.text[2])\n",
    "print(rawTweetData.text[3])\n",
    "print(rawTweetData.text[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [à¸à¸±à¸à¸à¸µà¸à¸µà¹à¸à¹à¸²à¸§à¸à¸£à¸¡...\n",
      "1          [Donald, Trump, and, Roseanne, are, living]\n",
      "2                        [I, know, we, 're, all, numb]\n",
      "3                        [I, know, we, 're, all, numb]\n",
      "4    [Diaspora, culture, is, thinking, you, know, b...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Split tweet text word by word (tokenize)\n",
    "tokenizedTweet = rawTweetData.apply(lambda row: word_tokenize(row['text']), axis=1)\n",
    "print(tokenizedTweet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [à¸à¸±à¸à¸à¸µà¸à¸µà¹à¸à¹à¸²à¸§à¸à¸£à¸¡...\n",
      "1                    [Donald, Trump, Roseanne, living]\n",
      "2                                 [I, know, 're, numb]\n",
      "3                                 [I, know, 're, numb]\n",
      "4    [Diaspora, culture, thinking, know, better, pe...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#remove stopwords( words that don't have any positive/negative meanings)\n",
    "filteredTweet = tokenizedTweet.apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n",
    "print(filteredTweet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
